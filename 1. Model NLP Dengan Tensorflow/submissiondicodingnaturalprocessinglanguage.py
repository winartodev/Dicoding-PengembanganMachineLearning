# -*- coding: utf-8 -*-
"""SubmissionDicodingNaturalProcessingLanguage

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sG7EjIsBqJ0M5lmb1wQggbXcf8MSfRDv
"""

from google.colab import drive 

drive.mount('/content/drive')

import re 
import string

import pandas as pd
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup

from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

path = '/content/drive/MyDrive/Colab Notebooks/IMDB_Dataset.csv'

df = pd.read_csv(path)

df.head()

category = pd.get_dummies(df.sentiment)
df = pd.concat([df, category], axis=1)
df = df.drop(columns='sentiment')
df

df_new = df[:2500]

df_new.shape

class TextPreprocessing():
  def case_folding(self, text):
    text = BeautifulSoup(text, 'html.parser').get_text()
    text = text.lower() # ubah text menjadi huruf kecil
    text = text.strip() # menghapus wihte space pada awal kalimat
    text = re.sub(r"\d+", "", text) # menghapus angka yang tidak relevan
    text = text.translate(str.maketrans("","",string.punctuation)) # menghapus tanda baca
    return text

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end (self, epoch, logs={}):
    if (logs.get('accuracy') > 0.85 and logs.get('val_accuracy') > 0.85):
      keys = list(logs.keys())
      print("End epoch {} of training; got log keys: {}".format(epoch, keys))
      self.model.stop_training = True    

my_callbacks = myCallback()
text_preprocessing = TextPreprocessing()

df_new['review'] = df_new['review'].apply(text_preprocessing.case_folding).values

df_new

review = df_new['review'].values
sentiment = df_new[['negative', 'positive']].values

X_train, X_test, y_train, y_test = train_test_split(review, sentiment, test_size=0.2)

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(X_train) 
tokenizer.fit_on_texts(X_test)
 
sequence_train = tokenizer.texts_to_sequences(X_train)
sequence_test = tokenizer.texts_to_sequences(X_test)
 
padded_train = pad_sequences(sequence_train) 
padded_test = pad_sequences(sequence_test)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index), output_dim=16),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.5)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])

history = model.fit(padded_train, y_train, epochs=15, 
                    validation_data=(padded_test, y_test), callbacks=[my_callbacks])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower left')
plt.show()

