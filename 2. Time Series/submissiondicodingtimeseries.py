# -*- coding: utf-8 -*-
"""SubmissionDicodingTimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1849HTUi2y-JIr1eqMOZjDUwOrEXc3kT5
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import pandas_datareader as web
import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf

# %matplotlib inline
plt.style.use('seaborn')

df = web.DataReader('BBRI.JK',data_source='yahoo', start='2012-01-01', end='2021-02-17')

df.head()

print(df.shape)

df.info()

plt.figure(figsize=(20, 10))
plt.plot(df['Close'])
plt.title('Close Price History in BBRI', fontsize=20)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Stock Price in (Rp)', fontsize=14)
plt.show()

df.head()

df_close = df.filter(['Close'])
df_close_values = df_close.values
df_close_values

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(df_close_values)
scaled_data = scaled_data.flatten()

X_train, X_test = train_test_split(scaled_data, test_size=0.2)

X_train

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(X_train, window_size=60, batch_size=100, shuffle_buffer=1000)

model = tf.keras.models.Sequential([tf.keras.layers.LSTM(60, return_sequences=True),
                                    tf.keras.layers.LSTM(60),
                                    tf.keras.layers.Dense(30, activation='relu'),
                                    tf.keras.layers.Dense(10, activation='relu'),
                                    tf.keras.layers.Dense(1)])

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.Huber(),
              metrics=['mae'])
history = model.fit(train_set, epochs=50, verbose=1)

